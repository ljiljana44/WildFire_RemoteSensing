{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameters\n",
    "PATCH_SIZE = 12\n",
    "BANDS = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', \n",
    "         'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "MAX_PATCHES_PER_FILE = 100  # You can adjust based on memory\n",
    "\n",
    "def extract_patches_from_file(file_path, label, max_patches):\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    \n",
    "    # Keep only required bands\n",
    "    ds = ds[[b for b in BANDS if b in ds.data_vars]]\n",
    "    \n",
    "    if \"t\" in ds.dims:\n",
    "        ds = ds.isel(t=0)\n",
    "    \n",
    "    da = ds.to_array().transpose(\"y\", \"x\", \"variable\")  # shape: (H, W, 12)\n",
    "    data = da.values.astype(np.float32)\n",
    "    \n",
    "    # Pad if necessary to ensure full patches\n",
    "    h, w, c = data.shape\n",
    "    if h < PATCH_SIZE or w < PATCH_SIZE:\n",
    "        return [], []\n",
    "\n",
    "    patches = []\n",
    "    labels = []\n",
    "    \n",
    "    # Randomly sample patch locations\n",
    "    for _ in range(max_patches):\n",
    "        y = np.random.randint(0, h - PATCH_SIZE + 1)\n",
    "        x = np.random.randint(0, w - PATCH_SIZE + 1)\n",
    "        patch = data[y:y+PATCH_SIZE, x:x+PATCH_SIZE, :]\n",
    "\n",
    "        if not np.isnan(patch).any():\n",
    "            patches.append(patch)\n",
    "            labels.append(label)\n",
    "\n",
    "    return patches, labels\n",
    "\n",
    "\n",
    "def build_datacube_dataset(folder_path):\n",
    "    all_patches = []\n",
    "    all_labels = []\n",
    "\n",
    "    files = sorted(glob(os.path.join(folder_path, \"*.nc\")))\n",
    "    print(f\"Found {len(files)} files.\")\n",
    "\n",
    "    for file_path in tqdm(files):\n",
    "        label = 1 if \"after\" in os.path.basename(file_path).lower() else 0\n",
    "        patches, labels = extract_patches_from_file(file_path, label, MAX_PATCHES_PER_FILE)\n",
    "        all_patches.extend(patches)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    X = np.array(all_patches)  # shape: (N, 12, 12, 12)\n",
    "    y = np.array(all_labels)   # shape: (N,)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd9356",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = build_datacube_dataset(\"datacubes_2024\")\n",
    "print(\"X shape:\", X.shape)  # (samples, 12, 12, 12)\n",
    "print(\"y shape:\", y.shape)  # (samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9320893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X shape: (N, 12, 12, 12), y shape: (N,)\n",
    "# Reshape if needed: TensorFlow prefers channels-last: (samples, height, width, depth)\n",
    "X = X.astype(\"float32\")\n",
    "y = y.astype(\"int32\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)) \\\n",
    "    .shuffle(buffer_size=1000) \\\n",
    "    .batch(BATCH_SIZE) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)) \\\n",
    "    .batch(BATCH_SIZE) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dbb4fd",
   "metadata": {},
   "source": [
    "<h1>2d cnn</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b511e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(12, 12, 12)),  # (H, W, Bands)\n",
    "    \n",
    "    layers.Conv2D(32, kernel_size=3, activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    layers.Conv2D(64, kernel_size=3, activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc84ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=10  # You can increase this\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd66ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "import os\n",
    "\n",
    "# SETTINGS\n",
    "PATCH_SIZE = 12\n",
    "BANDS = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06',\n",
    "         'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "\n",
    "def classify_blocks_with_model(nc_file_path, model, output_tiff_path):\n",
    "    # Load the dataset\n",
    "    ds = xr.open_dataset(nc_file_path)\n",
    "    ds = ds[[b for b in BANDS if b in ds.data_vars]]\n",
    "    if \"t\" in ds.dims:\n",
    "        ds = ds.isel(t=0)\n",
    "\n",
    "    # Create 3D array (H, W, Bands)\n",
    "    da = ds.to_array().transpose(\"y\", \"x\", \"variable\")\n",
    "    data = da.values.astype(np.float32)\n",
    "\n",
    "    height, width, bands = data.shape\n",
    "    if height < PATCH_SIZE or width < PATCH_SIZE:\n",
    "        raise ValueError(\"Image too small for one patch.\")\n",
    "\n",
    "    # Compute how many full patches fit\n",
    "    n_y = height // PATCH_SIZE\n",
    "    n_x = width // PATCH_SIZE\n",
    "\n",
    "    # Create block grid for output\n",
    "    prediction_map = np.full((n_y, n_x), fill_value=-1, dtype=np.int8)\n",
    "\n",
    "    blocks = []\n",
    "    coords = []\n",
    "\n",
    "    for i in range(n_y):\n",
    "        for j in range(n_x):\n",
    "            y = i * PATCH_SIZE\n",
    "            x = j * PATCH_SIZE\n",
    "            patch = data[y:y+PATCH_SIZE, x:x+PATCH_SIZE, :]\n",
    "\n",
    "            if not np.isnan(patch).any():\n",
    "                patch = patch[..., np.newaxis]  # (12, 12, 12, 1)\n",
    "                blocks.append(patch)\n",
    "                coords.append((i, j))\n",
    "\n",
    "    if not blocks:\n",
    "        raise ValueError(\"No valid patches found for classification.\")\n",
    "\n",
    "    # Convert to tensor\n",
    "    X_blocks = np.stack(blocks)  # (N, 12, 12, 12, 1)\n",
    "    predictions = model.predict(X_blocks, batch_size=32)\n",
    "    labels = (predictions.flatten() > 0.5).astype(np.uint8)\n",
    "\n",
    "    # Fill prediction map\n",
    "    for (i, j), label in zip(coords, labels):\n",
    "        prediction_map[i, j] = label\n",
    "\n",
    "    # Scale up to original resolution for GeoTIFF (nearest-neighbor repeat)\n",
    "    full_map = np.kron(prediction_map, np.ones((PATCH_SIZE, PATCH_SIZE), dtype=np.uint8))\n",
    "\n",
    "    # Build transform and save GeoTIFF\n",
    "    transform = from_origin(\n",
    "        float(ds.x[0]), float(ds.y[0]),\n",
    "        float(ds.x[1] - ds.x[0]),\n",
    "        float(ds.y[0] - ds.y[1])\n",
    "    )\n",
    "\n",
    "    with rasterio.open(\n",
    "        output_tiff_path,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=full_map.shape[0],\n",
    "        width=full_map.shape[1],\n",
    "        count=1,\n",
    "        dtype=rasterio.uint8,\n",
    "        crs=ds.rio.crs if hasattr(ds, 'rio') else 'EPSG:4326',\n",
    "        transform=transform,\n",
    "    ) as dst:\n",
    "        dst.write(full_map, 1)\n",
    "\n",
    "    print(f\"✅ Saved classified TIFF to: {output_tiff_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "#model = tf.keras.models.load_model(\"cnn_model_3d.h5\")\n",
    "\n",
    "# Classify patches in .nc and write TIFF\n",
    "classify_blocks_with_model(\"datacubes_2024/fire_226116_before.nc\", model, \"classified_2dblocks.tif\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a73f52",
   "metadata": {},
   "source": [
    "<h1>3D cnn</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf244db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model_3d = models.Sequential([\n",
    "    layers.Input(shape=(12, 12, 12, 1)),  # (depth, height, width, channels)\n",
    "    \n",
    "    layers.Conv3D(32, kernel_size=(3, 3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    \n",
    "    layers.Conv3D(64, kernel_size=(3, 3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification: burned vs non-burned\n",
    "])\n",
    "\n",
    "model_3d.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_3d.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape from (N, 12, 12, 12) → (N, 12, 12, 12, 1)\n",
    "X_train_3d = X_train[..., np.newaxis]\n",
    "X_test_3d = X_test[..., np.newaxis]\n",
    "\n",
    "train_ds_3d = tf.data.Dataset.from_tensor_slices((X_train_3d, y_train)) \\\n",
    "    .shuffle(1000) \\\n",
    "    .batch(BATCH_SIZE) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds_3d = tf.data.Dataset.from_tensor_slices((X_test_3d, y_test)) \\\n",
    "    .batch(BATCH_SIZE) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_3d.fit(\n",
    "    train_ds_3d,\n",
    "    validation_data=test_ds_3d,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741db0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_blocks_with_model(\"datacubes_2024/fire_226116_after.nc\", model_3d, \"classified_3dblocks_Fire.tif\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e12b6",
   "metadata": {},
   "source": [
    "<h2> attention based</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e9b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_attention_model(input_shape=(12, 12, 12)):\n",
    "    input_tensor = layers.Input(shape=input_shape)  # (H, W, Bands)\n",
    "\n",
    "    # Reshape to (tokens, features): 144 tokens, each 12D (spectral)\n",
    "    x = layers.Reshape((144, 12))(input_tensor)  # (batch, 144, 12)\n",
    "\n",
    "    # Positional embedding (learnable)\n",
    "    pos_embed = layers.Embedding(input_dim=144, output_dim=12)\n",
    "    positions = tf.range(start=0, limit=144, delta=1)\n",
    "    x += pos_embed(positions)\n",
    "\n",
    "    # Attention block\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.MultiHeadAttention(num_heads=4, key_dim=12)(x, x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Classification head\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = models.Model(inputs=input_tensor, outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d2564",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_attention_model()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train\n",
    "model.fit(train_ds, validation_data=test_ds, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07b323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_blocks_with_model(\"datacubes_2024/fire_226116_after.nc\", model, \"classified_attentio_Fire.tif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c89b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
