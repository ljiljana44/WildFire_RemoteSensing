{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b1f723",
   "metadata": {},
   "source": [
    "# üîß Check Runtime Environment\n",
    "Detects whether the notebook is running in Google Colab or locally, and sets the `base_dir` accordingly for file access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59db02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    base_dir = \"/content/drive/MyDrive/WildFire_RemoteSensing_workshop/WildFire_RemoteSensing/\"\n",
    "    print(f\"üìÇ Running in Colab, base_dir set to: {base_dir}\")\n",
    "else:\n",
    "    base_dir = \"\"  # Adjust as needed\n",
    "    print(f\"üñ•Ô∏è Running locally, base_dir set to: {base_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66de791",
   "metadata": {},
   "source": [
    "# üì¶ Install Dependencies\n",
    "Installs required Python packages (`tqdm`, `rasterio`) if not already available in the current environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5eb05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm rasterio "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d0b9e",
   "metadata": {},
   "source": [
    "# üìÅ Define Data Directory\n",
    "Specifies the directory containing the NetCDF (`.nc`) files that will be used to build the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ed8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = base_dir+\"datacubes_2024\"  # Change this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1413a95d",
   "metadata": {},
   "source": [
    "# üß† Import Libraries & Define Functions\n",
    "\n",
    "- Imports all necessary libraries\n",
    "- Defines the following steps:\n",
    "  - `get_nc_files()`: finds all `.nc` files in the data directory.\n",
    "  - `extract_pixels_and_labels()`: extracts pixel-level features and assigns labels (0 = \"before fire\", 1 = \"after fire\").\n",
    "  - `build_dataset()`: loops through files, extracts features, and stacks them into `X` and `y`.\n",
    "  - `save_dataset_to_csv()`: saves the resulting dataset as a CSV for reuse.\n",
    "- The process supports Sentinel-2 bands and removes invalid or missing data entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76952a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "# --- SETTINGS ---\n",
    " \n",
    "max_pixels_per_file = 1000\n",
    "\n",
    "# --- STEP 1: Collect all .nc files ---\n",
    "def get_nc_files(directory):\n",
    "    return glob.glob(os.path.join(directory, \"*.nc\"))\n",
    "\n",
    "# --- STEP 2: Load data and assign label ---\n",
    "def extract_pixels_and_labels1(file_path, label, max_pixels):\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Filter only numeric bands\n",
    "    numeric_bands = [var for var in ds.data_vars if np.issubdtype(ds[var].dtype, np.number)]\n",
    "    ds = ds[numeric_bands]  # Keep only numeric variables\n",
    "\n",
    "    # Convert to array and remove time dim\n",
    "    da =   ds.to_array().isel(t=0) .transpose(\"y\", \"x\", \"variable\")\n",
    "\n",
    "    # Extract and clean pixels\n",
    "    pixels = da.values.reshape(-1, da.shape[2]).astype(np.float32, copy=False)\n",
    "    pixels = pixels[~np.isnan(pixels).any(axis=1)]\n",
    "\n",
    "    # Limit to max pixels\n",
    "    np.random.shuffle(pixels)\n",
    "    return pixels[:max_pixels], np.full(min(len(pixels), max_pixels), label)\n",
    "def extract_pixels_and_labels(file_path, label, max_pixels):\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Define desired Sentinel-2 bands\n",
    "    desired_bands = [\n",
    "        'B01', 'B02', 'B03', 'B04', 'B05', 'B06',\n",
    "        'B07', 'B08', 'B8A', 'B09', 'B11', 'B12'\n",
    "    ]\n",
    "\n",
    "    # Filter only available & numeric desired bands\n",
    "    available_bands = [\n",
    "        band for band in desired_bands\n",
    "        if band in ds.data_vars and np.issubdtype(ds[band].dtype, np.number)\n",
    "    ]\n",
    "\n",
    "    if not available_bands:\n",
    "        print(f\"‚ö†Ô∏è No valid bands found in {file_path}, skipping.\")\n",
    "        return np.empty((0, len(desired_bands))), np.array([])\n",
    "\n",
    "    ds = ds[available_bands]\n",
    "\n",
    "    # Select first time slice if needed\n",
    "    if \"t\" in ds.dims:\n",
    "        ds = ds.isel(t=0)\n",
    "\n",
    "    # Convert to DataArray and reorder dimensions\n",
    "    da = ds.to_array().transpose(\"y\", \"x\", \"variable\")\n",
    "\n",
    "    # Flatten and clean\n",
    "    pixels = da.values.reshape(-1, da.shape[2]).astype(np.float32, copy=False)\n",
    "    pixels = pixels[~np.isnan(pixels).any(axis=1)]\n",
    "\n",
    "    # Sample subset\n",
    "    np.random.shuffle(pixels)\n",
    "    return pixels[:max_pixels], np.full(min(len(pixels), max_pixels), label)\n",
    "\n",
    "\n",
    "# --- STEP 3: Aggregate dataset ---\n",
    "def build_dataset(files, max_pixels):\n",
    "    X, y = [], []\n",
    "    for f in tqdm(files, desc=\"Processing .nc files\"):\n",
    "        label = 0 if \"before\" in f.lower() else 1\n",
    "        pixels, labels = extract_pixels_and_labels(f, label, max_pixels)\n",
    "        X.append(pixels)\n",
    "        y.append(labels)\n",
    "    return np.vstack(X), np.hstack(y)\n",
    "\n",
    "# Save features and labels to CSV\n",
    "def save_dataset_to_csv(X, y, output_path=base_dir+\"DATA/pixel_dataset.csv\"):\n",
    "    df = pd.DataFrame(X, columns=[f\"band_{i+1}\" for i in range(X.shape[1])])\n",
    "    df[\"label\"] = y\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Dataset saved to: {output_path}\")\n",
    "\n",
    "# --- MAIN ---\n",
    "\n",
    "files = get_nc_files(data_dir)\n",
    "X, y = build_dataset(files, max_pixels_per_file)\n",
    "save_dataset_to_csv(X, y,)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4396a676",
   "metadata": {},
   "source": [
    " <h1> Read from file</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564b5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(base_dir+'DATA/pixel_dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4066cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f17d7",
   "metadata": {},
   "source": [
    "# üìà Band Scatter Plot (B1 vs B12)\n",
    "Visualizes how two spectral bands (band_1 and band_12) vary across burn labels using a 2D scatter plot.\n",
    "\n",
    "Try changing the band numbers and see the corelation between bands values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2dc5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(df['band_1'],df['band_12'],alpha=0.5,c=df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "# Create figure with subplots\n",
    "n_bands = 12\n",
    "fig, axs = plt.subplots(n_bands, n_bands, figsize=(20, 20))\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Create custom colormap for burned/unburned\n",
    "cmap = plt.cm.colors.ListedColormap(['#1f77b4', '#ff7f0e'])  # Blue=unburned, Orange=burned\n",
    "\n",
    "# Plot each band combination\n",
    "for i in range(n_bands):\n",
    "    for j in range(n_bands):\n",
    "        ax = axs[i, j]\n",
    "        \n",
    "        # Hide upper triangle and diagonal\n",
    "        if i <= j:\n",
    "            ax.remove()\n",
    "            continue\n",
    "            \n",
    "        # Plot scatter points\n",
    "        scatter = ax.scatter(df['band_'+str(i+1)], df['band_'+str(j+1)], c=df['label'], cmap=cmap, \n",
    "                            alpha=0.6, s=10, edgecolors='none')\n",
    "        \n",
    "        # Axis labels\n",
    "        if i == n_bands-1:  # Bottom row\n",
    "            ax.set_xlabel(  'band_'+str(i+1))\n",
    "        if j == 0:  # Left column\n",
    "            ax.set_ylabel('band_'+str(j+1))\n",
    "            \n",
    "        # Remove ticks for readability\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "# Add colorbar\n",
    "# cbar = fig.colorbar(scatter, ax=axs, orientation='vertical', fraction=0.02)\n",
    "# cbar.set_ticks([0.25, 0.75])\n",
    "# cbar.set_ticklabels(['Unburned', 'Burned'])\n",
    "\n",
    "plt.suptitle('Band Relationship Scatter Matrix (Burned vs Unburned)', y=0.92)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea2eb5",
   "metadata": {},
   "source": [
    "# üåø NDVI vs üî• NBR Scatter\n",
    "Calculate NDVI (Normalized Difference Vegetation Index) and NBR (Normalized Burn Ratio) indices, then plot them to evaluate their ability to separate burned and unburned pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511dc1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi=(df['band_8']-df['band_4'])/(df['band_8']+df['band_4'])\n",
    "nbr=(df['band_9']-df['band_12'])/(df['band_9']+df['band_12'])\n",
    "plt.scatter(ndvi, nbr, c=df['label'], cmap=cmap, alpha=0.6, s=10, edgecolors='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd43c890",
   "metadata": {},
   "source": [
    "# üìâ NDVI and NBR Distributions\n",
    "Plots histograms of NDVI and NBR values separately for burned and unburned classes to visualize statistical separation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f7cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip values to valid ranges for visualization\n",
    "ndvi = np.clip(ndvi, -1, 1)\n",
    "nbr = np.clip(nbr, -1, 1)\n",
    "labels=df['label']\n",
    "# Separate NDVI and NBR by label\n",
    "ndvi_burned = ndvi[labels == 1]\n",
    "ndvi_unburned = ndvi[labels == 0]\n",
    "nbr_burned = nbr[labels == 1]\n",
    "nbr_unburned = nbr[labels == 0]\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "bins = 50\n",
    "\n",
    "# NDVI histogram\n",
    "axs[0].hist(ndvi_unburned, bins=bins, alpha=0.6, color='green', label='Unburned', density=True)\n",
    "axs[0].hist(ndvi_burned, bins=bins, alpha=0.6, color='red', label='Burned', density=True)\n",
    "axs[0].set_title('NDVI Distribution by Burn Status')\n",
    "axs[0].set_xlabel('NDVI')\n",
    "axs[0].set_ylabel('Density')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# NBR histogram\n",
    "axs[1].hist(nbr_unburned, bins=bins, alpha=0.6, color='green', label='Unburned', density=True)\n",
    "axs[1].hist(nbr_burned, bins=bins, alpha=0.6, color='red', label='Burned', density=True)\n",
    "axs[1].set_title('NBR Distribution by Burn Status')\n",
    "axs[1].set_xlabel('NBR')\n",
    "axs[1].set_ylabel('Density')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.suptitle('Distributions of NDVI and NBR for Burned vs Unburned Pixels', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5449e24",
   "metadata": {},
   "source": [
    "<h1>Create Dataset For classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1adf80",
   "metadata": {},
   "source": [
    "# üßπ Reload and Clean Dataset\n",
    "- Reloads pixel data.\n",
    "- Removes rows with any negative values to clean noise or invalid pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c775d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset\n",
    "\n",
    "df = pd.read_csv(base_dir+\"DATA/pixel_dataset.csv\")\n",
    "\n",
    "#Clean noise from data\n",
    "df = df[(df >= 0).all(axis=1)]\n",
    " \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96b71c",
   "metadata": {},
   "source": [
    "# üß™ Prepare Train/Test Sets\n",
    "- Splits data into features (`X`) and labels (`y`)\n",
    "- Applies stratified train/test split\n",
    "- Normalizes features using `StandardScaler` to prepare for model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a39cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(\"label\", axis=1).values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Data loaded and split:\")\n",
    "print(f\"  Train shape: {X_train.shape}\")\n",
    "print(f\"  Test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff740454",
   "metadata": {},
   "source": [
    "<h2> Logistic Regression</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f7ea1",
   "metadata": {},
   "source": [
    "## üîç Logistic Regression\n",
    "Evaluates a baseline linear classifier for binary classification of burned vs. unburned pixels.\n",
    "# üìà Train & Evaluate Logistic Regression\n",
    "- Trains a logistic regression model on the training set.\n",
    "- Outputs accuracy, confusion matrix, and classification report on test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Assuming X_train_scaled, X_test_scaled, y_train, y_test are ready\n",
    "\n",
    "# Initialize and train logistic regression\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"Logistic Regression Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dba6ef",
   "metadata": {},
   "source": [
    "# ü§ñ Define Multiple Classification Models\n",
    "Initializes several popular classifiers for comparison:\n",
    "- Random Forest\n",
    "- Decision Tree\n",
    "- K-Nearest Neighbors\n",
    "- Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37992b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=10, random_state=42),\n",
    "    \"Decision Tree Classifier\" : DecisionTreeClassifier( random_state=42),\n",
    "     \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=3),\n",
    "     \"Naive Bayes\": GaussianNB(),\n",
    " }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4acc7",
   "metadata": {},
   "source": [
    "# üß™ Train & Evaluate Models\n",
    "Fits each model on the training data and prints accuracy on the test set.\n",
    "(*Classification report optionally included but commented out.*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e770bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining and evaluating: {name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    #print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d422288",
   "metadata": {},
   "source": [
    "# üîç Hyperparameter Tuning with GridSearchCV\n",
    "- Defines a parameter grid for `RandomForestClassifier`.\n",
    "- Runs 5-fold cross-validation to find the best combination.\n",
    "- Evaluates the best model on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece08849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define model and parameter grid\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 80],\n",
    "    'max_depth': [None, 10 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "# Setup GridSearch\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best RF params:\", grid_search.best_params_)\n",
    "print(\"Best RF CV accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Use best estimator to predict test set\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf0820d",
   "metadata": {},
   "source": [
    "<h2>Stacking Clasifier</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3560ca4",
   "metadata": {},
   "source": [
    "## üß† Stacking Classifier\n",
    "Introduces an ensemble model that combines several base classifiers into a stronger meta-model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956d59f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "# Base classifiers\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svm', LinearSVC(    random_state=42)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "]\n",
    "\n",
    "# Meta-classifier\n",
    "meta_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_clf,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "# Train stacking ensemble\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_stack = stacking_clf.predict(X_test)\n",
    "print(\"Stacking Classifier Performance:\\n\", classification_report(y_test, y_pred_stack))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d882e0b",
   "metadata": {},
   "source": [
    "<h2> Use model to predict </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1376c292",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Use Model to Predict on New Image\n",
    "Describes application of trained model to classify all pixels in a new `.nc` image file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc40c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "import joblib  # for loading trained model\n",
    "import os\n",
    "\n",
    "def classify_nc_to_tiff(nc_file_path, model, scaler, output_tiff_path):\n",
    "    # Load dataset\n",
    "    ds = xr.open_dataset(nc_file_path)\n",
    "    \n",
    "    # Select required bands\n",
    "    desired_bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06',\n",
    "                     'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "    ds = ds[[band for band in desired_bands if band in ds.data_vars]]\n",
    "    \n",
    "    if \"t\" in ds.dims:\n",
    "        ds = ds.isel(t=0)\n",
    "    \n",
    "    # Convert to DataArray\n",
    "    da = ds.to_array().transpose(\"y\", \"x\", \"variable\")\n",
    "    \n",
    "    # Save original shape and coordinates\n",
    "    height, width = da.shape[0], da.shape[1]\n",
    "    coords = ds.coords\n",
    "    \n",
    "    # Reshape to (n_pixels, n_bands)\n",
    "    pixels = da.values.reshape(-1, da.shape[2])\n",
    "    nan_mask = np.isnan(pixels).any(axis=1)\n",
    "\n",
    "    # Filter valid pixels\n",
    "    valid_pixels = pixels[~nan_mask]\n",
    "    valid_pixels_scaled = scaler.transform(valid_pixels)\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(valid_pixels_scaled)\n",
    "\n",
    "    # Reconstruct full classification map\n",
    "    full_pred = np.full((pixels.shape[0],), fill_value=-1, dtype=np.int16)\n",
    "    full_pred[~nan_mask] = predictions\n",
    "    classification_map = full_pred.reshape((height, width))\n",
    "\n",
    "    # Save as GeoTIFF\n",
    "    transform = from_origin(\n",
    "        float(ds.x[0]), float(ds.y[0]), \n",
    "        float(ds.x[1] - ds.x[0]), \n",
    "        float(ds.y[0] - ds.y[1])\n",
    "    )\n",
    "    \n",
    "    with rasterio.open(\n",
    "        output_tiff_path,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=rasterio.int16,\n",
    "        crs=ds.rio.crs if hasattr(ds, \"rio\") else \"EPSG:4326\",  # fallback CRS\n",
    "        transform=transform,\n",
    "    ) as dst:\n",
    "        dst.write(classification_map, 1)\n",
    "\n",
    "    print(f\"‚úÖ Saved classification map to: {output_tiff_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746abfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_nc_to_tiff(base_dir+\"datacubes_2024/fire_224248_after.nc\", stacking_clf, scaler, base_dir+\"classified_map.tif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c401a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_nc_to_tiff(base_dir+\"datacubes_2024/fire_226116_before.nc\", stacking_clf, scaler, base_dir+\"classified_map_bef.tif\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2164e",
   "metadata": {},
   "source": [
    "# ‚úÖ Classify and Save New Image\n",
    "Uses the trained stacking classifier to predict land cover (burned/unburned) from a new `.nc` file and saves the result as a `.tif` image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05383f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "with rasterio.open(base_dir+\"classified_map_bef.tif\") as src:\n",
    "    image = src.read(1)  # Read the first band\n",
    "    plt.imshow(image, cmap='viridis')\n",
    "    plt.title(\"Predicted Regression Output\")\n",
    "    plt.colorbar(label=\"Predicted Value\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c046df",
   "metadata": {},
   "source": [
    "<h2>Lazy Clasifier</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07573e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%pip install lazypredict\n",
    "\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "# Initialize LazyClassifier\n",
    "# Consider reducing predictions per model and setting ignore_warnings=True\n",
    "# for faster execution on potentially smaller datasets\n",
    "lazy_clf = LazyClassifier(verbose=0, ignore_warnings=False, custom_metric=None)\n",
    "\n",
    "# Fit the models\n",
    "models, predictions = lazy_clf.fit(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "print(\"\\n--- LazyClassifier Results ---\")\n",
    "models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
