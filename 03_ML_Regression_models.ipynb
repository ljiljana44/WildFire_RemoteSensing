{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cecf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    base_dir = \"/content/drive/MyDrive/WildFire_RemoteSensing_workshop/WildFire_RemoteSensing/\"\n",
    "    print(f\"üìÇ Running in Colab, base_dir set to: {base_dir}\")\n",
    "else:\n",
    "    base_dir = \"\"  # Adjust as needed\n",
    "    print(f\"üñ•Ô∏è Running locally, base_dir set to: {base_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b34fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "# Load shapefile\n",
    " \n",
    "\n",
    "import pandas as pd\n",
    " \n",
    "base_dir=\"\"\n",
    "# Filter by country and date range\n",
    "gdf = gpd.read_file(base_dir+\"effis_layer/fire_records.csv\")\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "max_pixels_per_file=10000\n",
    "data_dir=base_dir+\"datacubes_2024\"\n",
    "def extract_pixels_and_labels(file_path, label, max_pixels):\n",
    "    \n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Define desired Sentinel-2 bands\n",
    "    desired_bands = [\n",
    "        'B01', 'B02', 'B03', 'B04', 'B05', 'B06',\n",
    "        'B07', 'B08', 'B8A', 'B09', 'B11', 'B12'\n",
    "    ]\n",
    "\n",
    "    # Filter only available & numeric desired bands\n",
    "    available_bands = [\n",
    "        band for band in desired_bands\n",
    "        if band in ds.data_vars and np.issubdtype(ds[band].dtype, np.number)\n",
    "    ]\n",
    "\n",
    "    if not available_bands:\n",
    "        print(f\"‚ö†Ô∏è No valid bands found in {file_path}, skipping.\")\n",
    "        return np.empty((0, len(desired_bands))), np.array([])\n",
    "\n",
    "    ds = ds[available_bands]\n",
    "\n",
    "    # Select first time slice if needed\n",
    "    if \"t\" in ds.dims:\n",
    "        ds = ds.isel(t=0)\n",
    "\n",
    "    # Convert to DataArray and reorder dimensions\n",
    "    da = ds.to_array().transpose(\"y\", \"x\", \"variable\")\n",
    "\n",
    "    # Flatten and clean\n",
    "    pixels = da.values.reshape(-1, da.shape[2]).astype(np.float32, copy=False)\n",
    "    pixels = pixels[~np.isnan(pixels).any(axis=1)]\n",
    "\n",
    "    # Sample subset\n",
    "    np.random.shuffle(pixels)\n",
    "    return pixels[:max_pixels], np.full(min(len(pixels), max_pixels), label)\n",
    "def build_dataset(gdf, max_pixels):\n",
    "    X, y = [], []\n",
    "    for idx,row in gdf.iterrows():\n",
    "        id = row['id']\n",
    "        file_path=data_dir+'/'+ f\"fire_{id}_before.nc\"\n",
    "        \n",
    "\n",
    "        label = row['AREA_HA']#1 if \"after\" in os.path.basename(file_path).lower() else 0\n",
    "        pixels, labels = extract_pixels_and_labels(file_path, label, max_pixels)\n",
    "        X.append(pixels)\n",
    "        y.append(labels)\n",
    "    return np.vstack(X), np.hstack(y)\n",
    " \n",
    " \n",
    "X, y = build_dataset(gdf, max_pixels_per_file)\n",
    "df = pd.DataFrame(X, columns=[f\"band_{i+1}\" for i in range(X.shape[1])])\n",
    "df[\"label\"] = y\n",
    "df.to_csv(base_dir+'DATA/pixel_dataset_ba.csv', index=False)\n",
    "print(\"‚úÖ Dataset saved   \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63414b95",
   "metadata": {},
   "source": [
    "<h2>Read from file </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset\n",
    "\n",
    "df = pd.read_csv(base_dir+\"DATA/pixel_dataset_ba.csv\")\n",
    "\n",
    "#Clean noise from data\n",
    "df = df[(df >= 0).all(axis=1)]\n",
    " \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a049a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(\"label\", axis=1).values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "''''''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Data loaded and split:\")\n",
    "print(f\"  Train shape: {X_train.shape}\")\n",
    "print(f\"  Test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a178fc93",
   "metadata": {},
   "source": [
    "<h3> Linear Regression </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea47f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    " \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Load Data ===\n",
    " \n",
    "\n",
    "# === Linear Regression ===\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === Predict and Evaluate ===\n",
    "y_pred = model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"  R¬≤ score: {r2:.4f}\")\n",
    "print(f\"  MSE:  {mse:.4f}\")\n",
    "print(f\"  MAE:  {mae:.4f}\")\n",
    " \n",
    "    \n",
    "predictions = model.predict(X_test) \n",
    "\n",
    "# Optional: compare predictions vs true values\n",
    "plt.scatter(y_test, predictions, alpha=0.5)\n",
    "plt.xlabel(\"True Burned Area\")\n",
    "plt.ylabel(\"Predicted Burned Area\")\n",
    "plt.title(\"Prediction vs Ground Truth\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eada9b20",
   "metadata": {},
   "source": [
    "<h2> Polynomial Regression </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830af52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    " \n",
    "# === Generate polynomial features ===\n",
    " \n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# === Fit linear regression on polynomial features ===\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# === Predict and evaluate ===\n",
    "y_pred = model.predict(X_test_poly)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Polynomial Regression R¬≤ score: {r2:.4f}\")\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel(\"True Burned Area\")\n",
    "plt.ylabel(\"Predicted Burned Area\")\n",
    "plt.title(\"Prediction vs Ground Truth\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29291950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mlpr = MLPRegressor(max_iter=50, \n",
    "                    hidden_layer_sizes= tuple(100 for _ in range(3)), \n",
    "                    solver='adam',\n",
    "                    learning_rate='adaptive')\n",
    "\n",
    "mlpr.fit(X_train, y_train)\n",
    "predicted = mlpr.predict(X_test)\n",
    "parameters = mlpr.get_params()\n",
    "test_mse = mean_squared_error(y_test, predicted)\n",
    "print(parameters)\n",
    "print(test_mse)\n",
    "\n",
    "predictions = mlpr.predict(X_test) \n",
    "\n",
    "# Optional: compare predictions vs true values\n",
    "plt.scatter(y_test, predictions, alpha=0.5)\n",
    "plt.xlabel(\"True Burned Area\")\n",
    "plt.ylabel(\"Predicted Burned Area\")\n",
    "plt.title(\"Prediction vs Ground Truth\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deed286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predictions = mlpr.predict(X_test) \n",
    "\n",
    "# Optional: compare predictions vs true values\n",
    "plt.scatter(y_test, predictions, alpha=0.5)\n",
    "plt.xlabel(\"True Burned Area\")\n",
    "plt.ylabel(\"Predicted Burned Area\")\n",
    "plt.title(\"Prediction vs Ground Truth\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b21f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " \n",
    "# === MODEL ===\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(12,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1)  # Linear output for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# === TRAINING ===\n",
    "history = model.fit(\n",
    "    X_train , y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# === EVALUATE ===\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"\\n‚úÖ Test MAE: {mae:.2f}\")\n",
    "\n",
    "# === PLOT TRAINING LOSS ===\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training History\")\n",
    "plt.show()\n",
    "\n",
    "# === PREDICT ===\n",
    "predictions = model.predict(X_test).flatten()\n",
    "\n",
    "# Optional: compare predictions vs true values\n",
    "plt.scatter(y_test, predictions, alpha=0.5)\n",
    "plt.xlabel(\"True Burned Area\")\n",
    "plt.ylabel(\"Predicted Burned Area\")\n",
    "plt.title(\"Prediction vs Ground Truth\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a4bdfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
